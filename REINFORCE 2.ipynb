{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "gym.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility and testing, we'll need to set all this seed stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "g = torch.Generator().manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [docs](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "## Action Space\n",
    "There are four discrete actions available:\n",
    "\n",
    "0: do nothing\n",
    "\n",
    "1: fire left orientation engine\n",
    "\n",
    "2: fire main engine\n",
    "\n",
    "3: fire right orientation engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Observation Space\n",
    "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.shape)\n",
    "env.observation_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "- is increased/decreased the closer/further the lander is to the landing pad.\n",
    "- is increased/decreased the slower/faster the lander is moving.\n",
    "- is decreased the more the lander is tilted (angle not horizontal).\n",
    "- is increased by 10 points for each leg that is in contact with the ground.\n",
    "- is decreased by 0.03 points each frame a side engine is firing.\n",
    "- is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "An episode is considered a solution if it scores at least 200 points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset(seed=SEED)\n",
    "observation, reward, terminated, truncated, info = env.step(0)\n",
    "reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Objective\n",
    "\n",
    "Our performance measure: \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E} \\left[ \\sum_{t=0}^{T-1}r_{t+1} \\right]\n",
    "$$\n",
    "\n",
    "and our update rule:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\frac{\\partial}{\\partial \\theta} J(\\theta)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_a q_\\pi(S_t, a) \\nabla \\pi (a | S_t, \\theta) \\right]\n",
    "$$\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_a \\pi (a | S_t, \\theta) q_\\pi(S_t, a) \\frac{\\nabla \\pi (a | S_t, \\theta)}{\\pi (a | S_t, \\theta)} \\right]\n",
    "$$\n",
    "\n",
    "If we sample $A_t \\sim \\pi$, then we just replace the expectation over $a$ with the sample $A_t$. So we're doing this swap from expectation to sample:\n",
    "$$\n",
    "\\sum_a \\pi (a | S_t, \\theta) q_\\pi(S_t, a) \\rightarrow q_\\pi(S_t, A_t)\n",
    "$$\n",
    "which then simplifies $\\nabla J(\\theta)$ to\n",
    "\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ q_\\pi(S_t, A_t) \\frac{\\nabla \\pi (A_t | S_t, \\theta)}{\\pi (A_t | S_t, \\theta)} \\right]\n",
    "$$\n",
    "\n",
    "and by $\\mathbb{E}_\\pi [G_t | S_t, A_t] = q_\\pi (S_t, A_t)$, (add note to explain how we get G_t)\n",
    "\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ G_t \\frac{\\nabla \\pi (A_t | S_t, \\theta)}{\\pi (A_t | S_t, \\theta)} \\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make one more simplification: note that $\\nabla \\ln x = \\frac{\\nabla x}{x}$, so\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ G_t \\nabla \\ln \\pi (A_t | S_t, \\theta) \\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can actually calculate the value in brackets at each time step, and can then use it to update $\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\alpha G_t \\nabla \\ln \\pi(A_t | S_t, \\theta_t)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to generate an episode $S_0, A_0, R_1,...,S_{T-1}, A_{T-1}, R_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# initialize policy network\n",
    "# takes in a state, determines the next action\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, action_size, seed=None):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(obs_size, action_size, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layer(x)\n",
    "        return torch.log(F.softmax(logits, dim=-1) + 1e-9)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        log_probs = self.forward(state).squeeze(0)\n",
    "        probs = torch.exp(log_probs)\n",
    "        action = torch.multinomial(probs, 1, generator=torch.Generator().manual_seed(SEED))\n",
    "        return action.item(), log_probs[action]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a full episode of the moon landing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env=env, seed=None):\n",
    "    observation, _ = env.reset(seed=seed)\n",
    "\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    actions = []\n",
    "    observations = []\n",
    "\n",
    "    while True:\n",
    "        action, _log_probs = model.act(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(_log_probs)\n",
    "        observations.append(observation)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    return rewards, log_probs, actions, np.array(observations)\n",
    "\n",
    "model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "rewards, log_probs, actions, observations = run_episode(model)\n",
    "\n",
    "# print(rewards.shape)\n",
    "# print(len(log_probs))\n",
    "# print(actions)\n",
    "# print(observations)\n",
    "\n",
    "del model, rewards, log_probs, actions, observations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `log_probs` is an array of `torch.tensor`. We need this to be the case, because we have to run `.backward` on it later to calculate the gradients. Without doing this, we can't really use pytorch to do the heavy lifting for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, at the end of the episode, we need to determine $G$, the expected reward at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=.99):\n",
    "    G = []\n",
    "    R = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        R = rewards[i] + gamma * R\n",
    "        G.insert(0, R)\n",
    "    return G\n",
    "\n",
    "# test it\n",
    "r = np.array([1,1,1,1,1])\n",
    "g = np.array(discount_rewards(r))\n",
    "t = np.array([4.90099501, 3.940399, 2.9701, 1.99, 1])\n",
    "assert np.allclose(g, t), \"Incorrect sum of discounted rewards\"\n",
    "del r, g, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    nan,     nan,     nan,     nan,     nan,     nan,  0.0000,  0.0008],\n",
      "        [    nan,     nan,     nan,     nan,     nan,     nan,  0.0000,  0.0016],\n",
      "        [    nan,     nan,     nan,     nan,     nan,     nan,  0.0000,  0.0000],\n",
      "        [    nan,     nan,     nan,     nan,     nan,     nan,  0.0000, -0.0016]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def update(model, log_probs, rewards, alpha=0.01, gamma=0.99, step=True):\n",
    "    \"\"\"\n",
    "        we add this step param because we want to be able to debug it later.\n",
    "    \"\"\"\n",
    "    # compute the discounted rewards\n",
    "    G = discount_rewards(rewards, gamma)\n",
    "\n",
    "    model.optimizer.zero_grad() # reset the gradients\n",
    "\n",
    "    before = {}\n",
    "    i = 0\n",
    "    for param in model.parameters():\n",
    "        before[i] = param.grad.clone() if param.grad is not None else 0.\n",
    "        i += 1\n",
    "\n",
    "    for t in range(len(G)):\n",
    "        log_prob = log_probs[t]\n",
    "        g = G[t]\n",
    "\n",
    "        log_prob.backward() # compute the gradients of log pi(A_t, S_t, theta) at each timestep\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad = alpha * gamma**t * g * param.grad # scale the gradients by the reward\n",
    "                param.grad = param.grad * -1. # flip the gradients (maximize the reward)\n",
    "        if step:\n",
    "            model.optimizer.step() # update the weights using the gradients\n",
    "\n",
    "    after = {}\n",
    "    i = 0\n",
    "    for param in model.parameters():\n",
    "        after[i] = param.grad.clone() if param.grad is not None else 0.\n",
    "        i += 1\n",
    "\n",
    "    total_grads = {}\n",
    "    for i in range(len(before)):\n",
    "        total_grads[i] = after[i] - before[i]\n",
    "\n",
    "    return total_grads\n",
    "\n",
    "# testing the update function\n",
    "model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "rewards, log_probs, actions, observations = run_episode(model)\n",
    "\n",
    "old = list(model.parameters())[0].clone()\n",
    "update(model, log_probs, rewards, alpha=1., gamma=1.) # no discounting, no scaling\n",
    "new = list(model.parameters())[0].clone()\n",
    "\n",
    "delta = new - old\n",
    "print(delta)\n",
    "\n",
    "left_leg_contact = (observations[:, -2]).sum() > 0\n",
    "right_leg_contact = (observations[:, -1]).sum() > 0\n",
    "\n",
    "left_leg_delta = delta[:, -2].sum(axis=0)\n",
    "right_leg_delta = delta[:, -1].sum(axis=0)\n",
    "\n",
    "# if there was no right leg contact, the right leg should have 0 delta\n",
    "assert left_leg_contact == True or left_leg_delta == 0.\n",
    "assert right_leg_contact == True or right_leg_delta == 0.\n",
    "\n",
    "# del model, rewards, log_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(note: move this up) So the thing that's confusing me about this is: how could we be getting no changes in some of the weights? If we analyze the weight matrix, what we're seeing is the following table of how much each weight is changing:\n",
    "\n",
    "|                      | x-coord | y-coord | x-vel | y-vel | angle | angular-vel | left-leg-contact | right-leg-contact |\n",
    "|----------------------|---------|---------|-------|-------|-------|-------------|------------------|-------------------|\n",
    "| action 0: no-op      |         |         |       |       |       |             |                  |                   |\n",
    "| action 1: fire left  |         |         |       |       |       |             |                  |                   |\n",
    "| action 2: fire main  |         |         |       |       |       |             |                  |                   |\n",
    "| action 3: fire right |         |         |       |       |       |             |                  |                   |\n",
    "\n",
    "It makes sense as to why the policy gradient would be 0 for the left and right leg contact, if in the given episode it never made contact (the lander spun upside-down).\n",
    "\n",
    "Now a policy with no priors will draw from the actions equally, and with 20 steps it's basically certain that we'll choose each of the four actions at least once. So that's what's confusing to me: how is it that we're basically guaranteed to try all the actions but yield 0 delta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll try it for a bunch of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "for i in range(1200):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at some point in training, we run an episode which totally goes off the rails, yielding a reward that is many standard deviations from the normal range. I think the sessions are also probably longer too, because the lunar lander is just wizzing around, yielding bad rewards and many timesteps. We can check this easily by plotting the number of steps in an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "for i in range(1500):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.clear()\n",
    "ax.plot(rewards_track)\n",
    "ax.set_title(\"Reward\")\n",
    "\n",
    "# add a smoothed version\n",
    "if len(rewards_track) > 100:\n",
    "    smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "    ax.plot(smoothed)\n",
    "\n",
    "# also plot the number of steps\n",
    "ax2.plot(steps_per_episode, color='red')\n",
    "\n",
    "display(fig)\n",
    "clear_output(wait=True)\n",
    "plt.pause(0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, so we need to cap our episode lengths. If we don't solve within say 1000 steps, it should just be a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env=env, max_steps=1000):\n",
    "    observation, _ = env.reset()\n",
    "\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    actions = []\n",
    "    observations = []\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        action, _log_probs = model.act(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(_log_probs)\n",
    "        observations.append(observation)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "\n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    return rewards, log_probs, actions, np.array(observations)\n",
    "\n",
    "model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "rewards, log_probs, actions, observations = run_episode(model)\n",
    "\n",
    "# print(rewards.shape)\n",
    "# print(len(log_probs))\n",
    "# print(actions)\n",
    "# print(observations)\n",
    "\n",
    "del model, rewards, log_probs, actions, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "\n",
    "ax.clear()\n",
    "ax.plot(rewards_track)\n",
    "ax.set_title(\"Reward\")\n",
    "\n",
    "# add a smoothed version\n",
    "if len(rewards_track) > 100:\n",
    "    smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "    ax.plot(smoothed)\n",
    "\n",
    "# also plot the number of steps\n",
    "ax2.plot(steps_per_episode, color='red')\n",
    "ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting: the policy that we're learning is now just learning to fly off without landing. The behavior we're learning is to not actually land, but to not crash. Note how the score is never breaking above 100, so it's likely never learning to land. We can check this as well by plotting the moving average of how many episodes end with both legs in contact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "landed = []\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "    landed += [observations[-1, -2:].sum() == 2]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "        ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "        # and the number of times we landed\n",
    "        ax3.set_title(\"Landed\")\n",
    "        if len(landed) > 20:\n",
    "            smoothed = [np.mean(landed[i:i+20]) for i in range(len(landed) - 20)]\n",
    "            ax3.plot(smoothed, color='green')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "\n",
    "ax.clear()\n",
    "ax.plot(rewards_track)\n",
    "ax.set_title(\"Reward\")\n",
    "\n",
    "# add a smoothed version\n",
    "if len(rewards_track) > 100:\n",
    "    smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "    ax.plot(smoothed)\n",
    "\n",
    "# also plot the number of steps\n",
    "ax2.plot(steps_per_episode, color='red')\n",
    "ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "# and the number of times we landed\n",
    "ax3.set_title(\"Landed\")\n",
    "if len(landed) > 20:\n",
    "    smoothed = [np.mean(landed[i:i+20]) for i in range(len(landed) - 20)]\n",
    "    ax3.plot(smoothed, color='green')\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it is working now! I speculate that if we add a large negative reward if we don't land in time, then we'll also train faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env=env, max_steps=1000, overstep_penalty=-100):\n",
    "    observation, _ = env.reset()\n",
    "\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    actions = []\n",
    "    observations = []\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        action, _log_probs = model.act(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(_log_probs)\n",
    "        observations.append(observation)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "        if steps >= max_steps:\n",
    "            rewards[-1] += overstep_penalty\n",
    "            break\n",
    "\n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    return rewards, log_probs, actions, np.array(observations)\n",
    "\n",
    "model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "rewards, log_probs, actions, observations = run_episode(model)\n",
    "\n",
    "# print(rewards.shape)\n",
    "# print(len(log_probs))\n",
    "# print(actions)\n",
    "# print(observations)\n",
    "\n",
    "del model, rewards, log_probs, actions, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "landed = []\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "    landed += [observations[-1, -2:].sum() == 2]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "        ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "        # and the number of times we landed\n",
    "        ax3.set_title(\"Landed\")\n",
    "        if len(landed) > 20:\n",
    "            smoothed = [np.mean(landed[i:i+20]) for i in range(len(landed) - 20)]\n",
    "            ax3.plot(smoothed, color='green')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "\n",
    "ax.clear()\n",
    "ax.plot(rewards_track)\n",
    "ax.set_title(\"Reward\")\n",
    "\n",
    "# add a smoothed version\n",
    "if len(rewards_track) > 100:\n",
    "    smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "    ax.plot(smoothed)\n",
    "\n",
    "# also plot the number of steps\n",
    "ax2.plot(steps_per_episode, color='red')\n",
    "ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "# and the number of times we landed\n",
    "ax3.set_title(\"Landed\")\n",
    "if len(landed) > 20:\n",
    "    smoothed = [np.mean(landed[i:i+20]) for i in range(len(landed) - 20)]\n",
    "    ax3.plot(smoothed, color='green')\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like adding a negative penalty was not all that impactful. \n",
    "\n",
    "Let's try creating a deeper policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# initialize policy network\n",
    "# takes in a state, determines the next action\n",
    "class BeefyPolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, action_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layer(x)\n",
    "        return torch.log(F.softmax(logits, dim=-1) + 1e-9)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        log_probs = self.forward(state).squeeze(0)\n",
    "        probs = torch.exp(log_probs)\n",
    "        action = torch.multinomial(probs, 1, generator=g)\n",
    "        return action.item(), log_probs[action]\n",
    "    \n",
    "model = BeefyPolicyNetwork(env.observation_space.shape[0], (env.action_space.n - env.action_space.start))\n",
    "action, log_prob = model.act(np.random.randn(8))\n",
    "log_prob.backward()\n",
    "\n",
    "old = list(model.parameters())[-1].clone()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.grad *= 1e9\n",
    "\n",
    "model.optimizer.step()\n",
    "\n",
    "new = list(model.parameters())[-1].clone()\n",
    "\n",
    "print(new - old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = BeefyPolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "landed = []\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "    landed += [observations[-1, -2:].sum() == 2]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "        ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "        # and the number of times we landed\n",
    "        ax3.set_title(\"Landed\")\n",
    "        if len(landed) > 30:\n",
    "            smoothed = [np.mean(landed[i:i+30]) for i in range(len(landed) - 30)]\n",
    "            ax3.plot(smoothed, color='green')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we're getting this bug: `RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 4]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).` I have a sense that this is caused by our `update` method.\n",
    "\n",
    "Yep, it's because we run `model.optimizer.step()` many times in a row. We need to do it only once, aka not in the for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_update(model, log_probs, rewards, alpha=.01, gamma=.99, step=True):\n",
    "    G = discount_rewards(rewards, gamma)\n",
    "\n",
    "    # can i reorder this so it still works? technically the updates don't have to be done in any order, \n",
    "    # so i don't need to use a for loop to iterate over t, i can try to run it in parallel.\n",
    "\n",
    "    model.optimizer.zero_grad() # reset the gradients\n",
    "\n",
    "    grads = []\n",
    "    for t in range(len(G)):\n",
    "        log_prob = log_probs[t]\n",
    "        g = G[t]\n",
    "\n",
    "        log_prob.backward() # compute the gradients of log pi(A_t, S_t, theta) at each timestep\n",
    "        \n",
    "        grad_t = {}\n",
    "        i = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_t[i] = -1. * alpha * gamma**t * g * param.grad # scale the gradients by the reward\n",
    "            i += 1\n",
    "        grads += [grad_t]\n",
    "\n",
    "    # sum all of the grads\n",
    "    total_grads = {k: sum(d[k] for d in grads) for k in list(grads[0].keys())}\n",
    "\n",
    "    i = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data = total_grads[i]\n",
    "        i += 1\n",
    "    \n",
    "    if step:\n",
    "        model.optimizer.step() # update the weights using the gradients\n",
    "    \n",
    "    return total_grads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `batch_update`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no practical way in pytorch to ensure the weights get initialized to the same random value every time, but we can store the weights and reload them whenever we test other models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain what's going on here. We need to be able to ensure that the gradients that pass when we use the `update` function are the same as the ones when we use the `batch_update` function. To do this, we initialize two models with the same weights, run them through the same episode with the same initial conditions, and we *don't* update the params using `model.optimizer.step()`, but instead just look at what the changes would be.\n",
    "\n",
    "We're making serious use of the `random.seed` stuff, and we need to to ensure that when we run the model, we get the same results each time, so that if we use the `batch_update` instead of `update`, we can be sure that the only change we introduced is the batching. See how we can run the cell below many times and we always get the same result? That's what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "test_model = PolicyNetwork(env.observation_space.shape[0], (env.action_space.n - env.action_space.start), seed=SEED)\n",
    "control_model = copy.deepcopy(test_model)\n",
    "\n",
    "c_rewards, c_log_probs, c_actions, c_observations = run_episode(control_model, seed=1)\n",
    "c_delta = update(control_model, c_log_probs, c_rewards, step=False)\n",
    "\n",
    "t_rewards, t_log_probs, t_actions, t_observations = run_episode(test_model, seed=1)\n",
    "t_delta = update(test_model, t_log_probs, t_rewards, step=False)\n",
    "\n",
    "for key in t_delta:\n",
    "    assert torch.allclose(c_delta[key], t_delta[key])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
