{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.29.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "\n",
    "gym.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility and testing, we'll need to set all this seed stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "g = torch.Generator().manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [docs](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "## Action Space\n",
    "There are four discrete actions available:\n",
    "\n",
    "0: do nothing\n",
    "\n",
    "1: fire left orientation engine\n",
    "\n",
    "2: fire main engine\n",
    "\n",
    "3: fire right orientation engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Observation Space\n",
    "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
       " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
       " 1.       ], (8,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space.shape)\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1449811454510836"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(seed=SEED)\n",
    "observation, reward, terminated, truncated, info = env.step(0)\n",
    "reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Objective\n",
    "\n",
    "Our performance measure: \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E} \\left[ \\sum_{t=0}^{T-1}r_{t+1} \\right]\n",
    "$$\n",
    "\n",
    "and our update rule:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\frac{\\partial}{\\partial \\theta} J(\\theta)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_a q_\\pi(S_t, a) \\nabla \\pi (a | S_t, \\theta) \\right]\n",
    "$$\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_a \\pi (a | S_t, \\theta) q_\\pi(S_t, a) \\frac{\\nabla \\pi (a | S_t, \\theta)}{\\pi (a | S_t, \\theta)} \\right]\n",
    "$$\n",
    "\n",
    "If we sample $A_t \\sim \\pi$, then we just replace the expectation over $a$ with the sample $A_t$. So we're doing this swap from expectation to sample:\n",
    "$$\n",
    "\\sum_a \\pi (a | S_t, \\theta) q_\\pi(S_t, a) \\rightarrow q_\\pi(S_t, A_t)\n",
    "$$\n",
    "which then simplifies $\\nabla J(\\theta)$ to\n",
    "\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ q_\\pi(S_t, A_t) \\frac{\\nabla \\pi (A_t | S_t, \\theta)}{\\pi (A_t | S_t, \\theta)} \\right]\n",
    "$$\n",
    "\n",
    "and by $\\mathbb{E}_\\pi [G_t | S_t, A_t] = q_\\pi (S_t, A_t)$, (add note to explain how we get G_t)\n",
    "\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ G_t \\frac{\\nabla \\pi (A_t | S_t, \\theta)}{\\pi (A_t | S_t, \\theta)} \\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make one more simplification: note that $\\nabla \\ln x = \\frac{\\nabla x}{x}$, so\n",
    "$$\n",
    "\\nabla J (\\theta) = \\mathbb{E}_\\pi \\left[ G_t \\nabla \\ln \\pi (A_t | S_t, \\theta) \\right]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can actually calculate the value in brackets at each time step, and can then use it to update $\\theta$:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\alpha G_t \\nabla \\ln \\pi(A_t | S_t, \\theta_t)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to generate an episode $S_0, A_0, R_1,...,S_{T-1}, A_{T-1}, R_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# initialize policy network\n",
    "# takes in a state, determines the next action\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, action_size, seed=None):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(obs_size, action_size, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, maximize=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layer(x)\n",
    "        return torch.log(F.softmax(logits, dim=-1) + 1e-9)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        log_probs = self.forward(state).squeeze(0)\n",
    "        probs = torch.exp(log_probs)\n",
    "        action = torch.multinomial(probs, 1, generator=torch.Generator().manual_seed(SEED))\n",
    "        return action.item(), log_probs[action]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a full episode of the moon landing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env=env, seed=None):\n",
    "    observation, _ = env.reset(seed=seed)\n",
    "\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    actions = []\n",
    "    observations = []\n",
    "\n",
    "    while True:\n",
    "        action, _log_probs = model.act(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(_log_probs)\n",
    "        observations.append(observation)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    return rewards, log_probs, actions, np.array(observations)\n",
    "\n",
    "model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "rewards, log_probs, actions, observations = run_episode(model)\n",
    "\n",
    "# print(rewards.shape)\n",
    "# print(len(log_probs))\n",
    "# print(actions)\n",
    "# print(observations)\n",
    "\n",
    "del model, rewards, log_probs, actions, observations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `log_probs` is an array of `torch.tensor`. We need this to be the case, because we have to run `.backward` on it later to calculate the gradients. Without doing this, we can't really use pytorch to do the heavy lifting for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, at the end of the episode, we need to determine $G$, the expected reward at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=.99):\n",
    "    G = []\n",
    "    R = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        R = rewards[i] + gamma * R\n",
    "        G.insert(0, R)\n",
    "    return G\n",
    "\n",
    "# test it\n",
    "r = np.array([1,1,1,1,1])\n",
    "g = np.array(discount_rewards(r))\n",
    "t = np.array([4.90099501, 3.940399, 2.9701, 1.99, 1])\n",
    "assert np.allclose(g, t), \"Incorrect sum of discounted rewards\"\n",
    "del r, g, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(model, log_probs, rewards, alpha=0.01, gamma=0.99, step=True):\n",
    "    \"\"\"\n",
    "        we add this step param because we want to be able to debug it later.\n",
    "    \"\"\"\n",
    "    # compute the discounted rewards\n",
    "    G = discount_rewards(rewards, gamma)\n",
    "\n",
    "    model.optimizer.zero_grad() # reset the gradients\n",
    "\n",
    "    before = {}\n",
    "    i = 0\n",
    "    for param in model.parameters():\n",
    "        before[i] = param.data.clone()\n",
    "        i += 1\n",
    "\n",
    "    updates = {}\n",
    "\n",
    "    for t in range(len(G)):\n",
    "        model.optimizer.zero_grad() # reset the gradients\n",
    "        log_prob = log_probs[t]\n",
    "        g = G[t]\n",
    "\n",
    "        t_before = {}\n",
    "        i = 0\n",
    "        for param in model.parameters():\n",
    "            t_before[i] = param.data.clone()\n",
    "            i += 1\n",
    "\n",
    "        log_prob.backward() # compute the gradients of log pi(A_t, S_t, theta) at each timestep\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad = alpha * gamma**t * g * param.grad # scale the gradients by the reward\n",
    "        \n",
    "        adam = copy.deepcopy(model.optimizer.state_dict()[\"state\"])\n",
    "        if step:\n",
    "            model.optimizer.step() # update the weights using the gradients\n",
    "\n",
    "        t_after = {}\n",
    "        i = 0\n",
    "        for param in model.parameters():\n",
    "            t_after[i] = param.data.clone()\n",
    "            i += 1\n",
    "\n",
    "        t_delta = {}\n",
    "        for i in range(len(t_before)):\n",
    "            t_delta[i] = t_after[i] - t_before[i]\n",
    "\n",
    "        updates[t] = {\n",
    "            \"before\": t_before,\n",
    "            \"grad\": {i: param.grad.clone() for i, param in enumerate(model.parameters())},\n",
    "            \"after\": t_after,\n",
    "            \"delta\": t_delta,\n",
    "            \"adam\": adam,\n",
    "        }\n",
    "        \n",
    "    after = {}\n",
    "    i = 0\n",
    "    for param in model.parameters():\n",
    "        after[i] = param.data.clone()\n",
    "        i += 1\n",
    "\n",
    "    total_delta = {}\n",
    "    for i in range(len(before)):\n",
    "        total_delta[i] = after[i] - before[i]\n",
    "\n",
    "    return before, after, total_delta, updates\n",
    "\n",
    "# testing the update function\n",
    "model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "rewards, log_probs, actions, observations = run_episode(model)\n",
    "\n",
    "old = list(model.parameters())[0].clone()\n",
    "before, after, delta, updates = update(model, log_probs, rewards, alpha=1., gamma=1.) # no discounting, no scaling\n",
    "new = list(model.parameters())[0].clone()\n",
    "\n",
    "t_delta = new - old\n",
    "assert torch.allclose(delta[0], t_delta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, `delta != grad` because of the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[-0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0.]])\n",
      "66\n",
      "{0: {'step': tensor(1.), 'exp_avg': tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]]), 'exp_avg_sq': tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3237, -0.1193, -0.1253, -0.3421, -0.2025,  0.0883, -0.0467, -0.2566],\n",
       "        [ 0.0083, -0.2415, -0.3000, -0.1947, -0.3094, -0.2251,  0.3534,  0.0668],\n",
       "        [ 0.1090, -0.3298, -0.2322, -0.1177,  0.0553, -0.3111, -0.1523, -0.2117],\n",
       "        [ 0.0010, -0.1316, -0.0245, -0.2396, -0.2427, -0.2063, -0.1210, -0.2791]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert torch.allclose(updates[0]['delta'][0], updates[0]['after'][0] - updates[0]['before'][0] )\n",
    "\n",
    "print(updates[1]['delta'][0])\n",
    "print(updates[1]['grad'][0])\n",
    "\n",
    "print(len(updates))\n",
    "print(updates[1]['adam'])\n",
    "\n",
    "# debugging adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "m = 0\n",
    "v = 0\n",
    "lr = 1e-3\n",
    "g = updates[0]['grad'][0]\n",
    "m = betas[0] * m + (1 - betas[0]) * g\n",
    "v = betas[1] * v + (1 - betas[1]) * (g ** 2)\n",
    "m_hat = m / (1 - betas[0])\n",
    "v_hat = v / (1 - betas[1])\n",
    "updates[0]['before'][0] - lr * m_hat / (torch.sqrt(v_hat) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sense making\n",
    "left_leg_contact = (observations[:, -2]).sum() > 0\n",
    "right_leg_contact = (observations[:, -1]).sum() > 0\n",
    "\n",
    "left_leg_delta = delta[0][:, -2].sum(axis=0)\n",
    "right_leg_delta = delta[0][:, -1].sum(axis=0)\n",
    "\n",
    "# if there was no right leg contact, the right leg should have 0 delta\n",
    "assert left_leg_contact == True or left_leg_delta == 0.\n",
    "assert right_leg_contact == True or right_leg_delta == 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(note: move this up) So the thing that's confusing me about this is: how could we be getting no changes in some of the weights? If we analyze the weight matrix, what we're seeing is the following table of how much each weight is changing:\n",
    "\n",
    "|                      | x-coord | y-coord | x-vel | y-vel | angle | angular-vel | left-leg-contact | right-leg-contact |\n",
    "|----------------------|---------|---------|-------|-------|-------|-------------|------------------|-------------------|\n",
    "| action 0: no-op      |         |         |       |       |       |             |                  |                   |\n",
    "| action 1: fire left  |         |         |       |       |       |             |                  |                   |\n",
    "| action 2: fire main  |         |         |       |       |       |             |                  |                   |\n",
    "| action 3: fire right |         |         |       |       |       |             |                  |                   |\n",
    "\n",
    "It makes sense as to why the policy gradient would be 0 for the left and right leg contact, if in the given episode it never made contact (the lander spun upside-down).\n",
    "\n",
    "Now a policy with no priors will draw from the actions equally, and with 20 steps it's basically certain that we'll choose each of the four actions at least once. So that's what's confusing to me: how is it that we're basically guaranteed to try all the actions but yield 0 delta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll try it for a bunch of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1200\u001b[39m):\n\u001b[1;32m     12\u001b[0m     rewards, log_probs, actions, observations \u001b[39m=\u001b[39m run_episode(model)\n\u001b[0;32m---> 13\u001b[0m     update(model, log_probs, rewards)\n\u001b[1;32m     15\u001b[0m     rewards_track \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39msum\u001b[39m(rewards)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mtolist()]\n\u001b[1;32m     17\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(model, log_probs, rewards, alpha, gamma, step)\u001b[0m\n\u001b[1;32m     26\u001b[0m     t_before[i] \u001b[39m=\u001b[39m param\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclone()\n\u001b[1;32m     27\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 29\u001b[0m log_prob\u001b[39m.\u001b[39;49mbackward() \u001b[39m# compute the gradients of log pi(A_t, S_t, theta) at each timestep\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/rl/.rl/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/projects/rl/.rl/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "for i in range(1200):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at some point in training, we run an episode which totally goes off the rails, yielding a reward that is many standard deviations from the normal range. I think the sessions are also probably longer too, because the lunar lander is just wizzing around, yielding bad rewards and many timesteps. We can check this easily by plotting the number of steps in an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "for i in range(1500):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.clear()\n",
    "ax.plot(rewards_track)\n",
    "ax.set_title(\"Reward\")\n",
    "\n",
    "# add a smoothed version\n",
    "if len(rewards_track) > 100:\n",
    "    smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "    ax.plot(smoothed)\n",
    "\n",
    "# also plot the number of steps\n",
    "ax2.plot(steps_per_episode, color='red')\n",
    "\n",
    "display(fig)\n",
    "clear_output(wait=True)\n",
    "plt.pause(0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, so we need to cap our episode lengths. If we don't solve within say 1000 steps, it should just be a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env=env, max_steps=1000):\n",
    "    observation, _ = env.reset()\n",
    "\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    actions = []\n",
    "    observations = []\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        action, _log_probs = model.act(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(_log_probs)\n",
    "        observations.append(observation)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "\n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    return rewards, log_probs, actions, np.array(observations)\n",
    "\n",
    "model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "rewards, log_probs, actions, observations = run_episode(model)\n",
    "\n",
    "# print(rewards.shape)\n",
    "# print(len(log_probs))\n",
    "# print(actions)\n",
    "# print(observations)\n",
    "\n",
    "del model, rewards, log_probs, actions, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5000\u001b[39m):\n\u001b[1;32m     14\u001b[0m     rewards, log_probs, actions, observations \u001b[39m=\u001b[39m run_episode(model)\n\u001b[0;32m---> 15\u001b[0m     update(model, log_probs, rewards)\n\u001b[1;32m     17\u001b[0m     rewards_track \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39msum\u001b[39m(rewards)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mtolist()]\n\u001b[1;32m     18\u001b[0m     steps_per_episode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(actions)]\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(model, log_probs, rewards, alpha, gamma, step)\u001b[0m\n\u001b[1;32m     26\u001b[0m     t_before[i] \u001b[39m=\u001b[39m param\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclone()\n\u001b[1;32m     27\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 29\u001b[0m log_prob\u001b[39m.\u001b[39;49mbackward() \u001b[39m# compute the gradients of log pi(A_t, S_t, theta) at each timestep\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/rl/.rl/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/projects/rl/.rl/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "\n",
    "ax.clear()\n",
    "ax.plot(rewards_track)\n",
    "ax.set_title(\"Reward\")\n",
    "\n",
    "# add a smoothed version\n",
    "if len(rewards_track) > 100:\n",
    "    smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "    ax.plot(smoothed)\n",
    "\n",
    "# also plot the number of steps\n",
    "ax2.plot(steps_per_episode, color='red')\n",
    "ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting: the policy that we're learning is now just learning to fly off without landing. The behavior we're learning is to not actually land, but to not crash. Note how the score is never breaking above 100, so it's likely never learning to land. We can check this as well by plotting the moving average of how many episodes end with both legs in contact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "landed = []\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "    landed += [observations[-1, -2:].sum() == 2]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "        ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "        # and the number of times we landed\n",
    "        ax3.set_title(\"Landed\")\n",
    "        if len(landed) > 20:\n",
    "            smoothed = [np.mean(landed[i:i+20]) for i in range(len(landed) - 20)]\n",
    "            ax3.plot(smoothed, color='green')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "\n",
    "ax.clear()\n",
    "ax.plot(rewards_track)\n",
    "ax.set_title(\"Reward\")\n",
    "\n",
    "# add a smoothed version\n",
    "if len(rewards_track) > 100:\n",
    "    smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "    ax.plot(smoothed)\n",
    "\n",
    "# also plot the number of steps\n",
    "ax2.plot(steps_per_episode, color='red')\n",
    "ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "# and the number of times we landed\n",
    "ax3.set_title(\"Landed\")\n",
    "if len(landed) > 20:\n",
    "    smoothed = [np.mean(landed[i:i+20]) for i in range(len(landed) - 20)]\n",
    "    ax3.plot(smoothed, color='green')\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it is working now! I speculate that if we add a large negative reward if we don't land in time, then we'll also train faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env=env, max_steps=1000, overstep_penalty=-100):\n",
    "    observation, _ = env.reset()\n",
    "\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    actions = []\n",
    "    observations = []\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        action, _log_probs = model.act(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(_log_probs)\n",
    "        observations.append(observation)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "        if steps >= max_steps:\n",
    "            rewards[-1] += overstep_penalty\n",
    "            break\n",
    "\n",
    "    rewards = torch.tensor(rewards)\n",
    "    \n",
    "    return rewards, log_probs, actions, np.array(observations)\n",
    "\n",
    "model = PolicyNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "rewards, log_probs, actions, observations = run_episode(model)\n",
    "\n",
    "# print(rewards.shape)\n",
    "# print(len(log_probs))\n",
    "# print(actions)\n",
    "# print(observations)\n",
    "\n",
    "del model, rewards, log_probs, actions, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = PolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "landed = []\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "    landed += [observations[-1, -2:].sum() == 2]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "        ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "        # and the number of times we landed\n",
    "        ax3.set_title(\"Landed\")\n",
    "        if len(landed) > 20:\n",
    "            smoothed = [np.mean(landed[i:i+20]) for i in range(len(landed) - 20)]\n",
    "            ax3.plot(smoothed, color='green')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "\n",
    "ax.clear()\n",
    "ax.plot(rewards_track)\n",
    "ax.set_title(\"Reward\")\n",
    "\n",
    "# add a smoothed version\n",
    "if len(rewards_track) > 100:\n",
    "    smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "    ax.plot(smoothed)\n",
    "\n",
    "# also plot the number of steps\n",
    "ax2.plot(steps_per_episode, color='red')\n",
    "ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "# and the number of times we landed\n",
    "ax3.set_title(\"Landed\")\n",
    "if len(landed) > 20:\n",
    "    smoothed = [np.mean(landed[i:i+20]) for i in range(len(landed) - 20)]\n",
    "    ax3.plot(smoothed, color='green')\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like adding a negative penalty was not all that impactful. \n",
    "\n",
    "Let's try creating a deeper policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# initialize policy network\n",
    "# takes in a state, determines the next action\n",
    "class BeefyPolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, action_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size, bias=False)\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layer(x)\n",
    "        return torch.log(F.softmax(logits, dim=-1) + 1e-9)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        log_probs = self.forward(state).squeeze(0)\n",
    "        probs = torch.exp(log_probs)\n",
    "        action = torch.multinomial(probs, 1, generator=g)\n",
    "        return action.item(), log_probs[action]\n",
    "    \n",
    "model = BeefyPolicyNetwork(env.observation_space.shape[0], (env.action_space.n - env.action_space.start))\n",
    "action, log_prob = model.act(np.random.randn(8))\n",
    "log_prob.backward()\n",
    "\n",
    "old = list(model.parameters())[-1].clone()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.grad *= 1e9\n",
    "\n",
    "model.optimizer.step()\n",
    "\n",
    "new = list(model.parameters())[-1].clone()\n",
    "\n",
    "print(new - old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = BeefyPolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "landed = []\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "    landed += [observations[-1, -2:].sum() == 2]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "        ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "        # and the number of times we landed\n",
    "        ax3.set_title(\"Landed\")\n",
    "        if len(landed) > 30:\n",
    "            smoothed = [np.mean(landed[i:i+30]) for i in range(len(landed) - 30)]\n",
    "            ax3.plot(smoothed, color='green')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we're getting this bug: `RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 4]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).` I have a sense that this is caused by our `update` method.\n",
    "\n",
    "Yep, it's because we run `model.optimizer.step()` many times in a row. We need to do it only once, aka not in the for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_update(model, log_probs, rewards, alpha=.01, gamma=.99, step=True):\n",
    "    G = discount_rewards(rewards, gamma)\n",
    "\n",
    "    # can i reorder this so it still works? technically the updates don't have to be done in any order, \n",
    "    # so i don't need to use a for loop to iterate over t, i can try to run it in parallel.\n",
    "    before = {}\n",
    "    i = 0\n",
    "    for param in model.parameters():\n",
    "        before[i] = param.data.clone()\n",
    "        i += 1\n",
    "\n",
    "    model.optimizer.zero_grad() # reset the gradients\n",
    "\n",
    "    grads = []\n",
    "    for t in range(len(G)):\n",
    "        log_prob = log_probs[t]\n",
    "        g = G[t]\n",
    "\n",
    "        log_prob.backward() # compute the gradients of log pi(A_t, S_t, theta) at each timestep\n",
    "        \n",
    "        grad_t = {}\n",
    "        i = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_t[i] = -1. * alpha * gamma**t * g * param.grad # scale the gradients by the reward\n",
    "            i += 1\n",
    "        grads += [grad_t]\n",
    "\n",
    "    # sum all of the grads\n",
    "    total_grads = {k: sum(d[k] for d in grads) for k in list(grads[0].keys())}\n",
    "\n",
    "    i = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data = total_grads[i]\n",
    "        i += 1\n",
    "    \n",
    "    if step:\n",
    "        model.optimizer.step() # update the weights using the gradients\n",
    "    \n",
    "    after = {}\n",
    "    i = 0\n",
    "    for param in model.parameters():\n",
    "        after[i] = param.data.clone()\n",
    "        i += 1\n",
    "    \n",
    "    total_delta = {}\n",
    "    for i in range(len(before)):\n",
    "        total_delta[i] = after[i] - before[i]\n",
    "    \n",
    "    return before, after, delta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `batch_update`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no practical way in pytorch to ensure the weights get initialized to the same random value every time, but we can store the weights and reload them whenever we test other models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain what's going on here. We need to be able to ensure that the gradients that pass when we use the `update` function are the same as the ones when we use the `batch_update` function. To do this, we initialize two models with the same weights, run them through the same episode with the same initial conditions, and we *don't* update the params using `model.optimizer.step()`, but instead just look at what the changes would be.\n",
    "\n",
    "We're making serious use of the `random.seed` stuff, and we need to to ensure that when we run the model, we get the same results each time, so that if we use the `batch_update` instead of `update`, we can be sure that the only change we introduced is the batching. See how we can run the cell below many times and we always get the same result? That's what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "test_model = PolicyNetwork(env.observation_space.shape[0], (env.action_space.n - env.action_space.start), seed=SEED)\n",
    "control_model = copy.deepcopy(test_model)\n",
    "\n",
    "c_rewards, c_log_probs, c_actions, c_observations = run_episode(control_model, seed=1)\n",
    "c_delta = update(control_model, c_log_probs, c_rewards, step=True)\n",
    "\n",
    "t_rewards, t_log_probs, t_actions, t_observations = run_episode(test_model, seed=1)\n",
    "t_delta = update(test_model, t_log_probs, t_rewards, step=True)\n",
    "\n",
    "print(t_delta)\n",
    "\n",
    "for key in t_delta:\n",
    "    assert torch.allclose(c_delta[key], t_delta[key])\n",
    "\n",
    "del test_model, control_model, c_rewards, c_log_probs, c_actions, c_observations, c_delta, t_rewards, t_log_probs, t_actions, t_observations, t_delta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so the reason this won't work:\n",
    "\n",
    "1. Let's define *won't work* as `batch_update` at the end of an episode will not yield the same result as `update` at the end of an episode.\n",
    "1. So the reason is this: in `update` we run `model.optimizer.step` after each time step in the episode. Our optimizer is the `torch.optim.Adam`, which relies on state information, specifically which time step we are on. So each time we run `step`, within `model.optimizer` there is a state that is updated, and then used. So there's a necessary side-effect in the `update` optimizer.\n",
    "1. This side-effect is *not* happening in `batch_update`, because we are not running `step` after each time step. So the state is not being updated, and the optimizer is not using the updated state.\n",
    "\n",
    "We can easily verify this by printing out the step of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PolicyNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m test_model \u001b[39m=\u001b[39m PolicyNetwork(env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], (env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn \u001b[39m-\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mstart), seed\u001b[39m=\u001b[39mSEED)\n\u001b[1;32m      4\u001b[0m control_model \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(test_model)\n\u001b[1;32m      6\u001b[0m c_rewards, c_log_probs, c_actions, c_observations \u001b[39m=\u001b[39m run_episode(control_model, seed\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PolicyNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "test_model = PolicyNetwork(env.observation_space.shape[0], (env.action_space.n - env.action_space.start), seed=SEED)\n",
    "control_model = copy.deepcopy(test_model)\n",
    "\n",
    "c_rewards, c_log_probs, c_actions, c_observations = run_episode(control_model, seed=1)\n",
    "c_before, c_after, c_delta, updates = update(control_model, c_log_probs, c_rewards, step=True)\n",
    "\n",
    "t_rewards, t_log_probs, t_actions, t_observations = run_episode(test_model, seed=1)\n",
    "t_before, t_after, t_delta = batch_update(test_model, t_log_probs, t_rewards, step=True)\n",
    "\n",
    "print(\"control\", control_model.optimizer.state_dict()['state'][0]['step'])\n",
    "print(\"test\", test_model.optimizer.state_dict()['state'][0]['step'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we swap out our optimizer from `Adam` to `SGD`, we probably could make it work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# initialize policy network\n",
    "# takes in a state, determines the next action\n",
    "class SGDPolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, action_size, seed=None):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(obs_size, action_size, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=1e-3, maximize=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layer(x)\n",
    "        return torch.log(F.softmax(logits, dim=-1) + 1e-9)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        log_probs = self.forward(state).squeeze(0)\n",
    "        probs = torch.exp(log_probs)\n",
    "        action = torch.multinomial(probs, 1, generator=torch.Generator().manual_seed(SEED))\n",
    "        return action.item(), log_probs[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = SGDPolicyNetwork(env.observation_space.shape[0], (env.action_space.n - env.action_space.start), seed=SEED)\n",
    "control_model = copy.deepcopy(test_model)\n",
    "c_rewards, c_log_probs, c_actions, c_observations = run_episode(control_model, seed=1)\n",
    "c_before, c_after, c_delta, updates = update(control_model, c_log_probs, c_rewards, step=True)\n",
    "\n",
    "t_rewards, t_log_probs, t_actions, t_observations = run_episode(test_model, seed=1)\n",
    "t_before, t_after, t_delta = batch_update(test_model, t_log_probs, t_rewards, step=True)\n",
    "\n",
    "print(updates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "network_shape = env.observation_space.shape[0], (env.action_space.n - env.action_space.start)\n",
    "model = SGDPolicyNetwork(*network_shape)\n",
    "\n",
    "rewards_track = []\n",
    "steps_per_episode = []\n",
    "landed = []\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,3,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "for i in range(5000):\n",
    "    rewards, log_probs, actions, observations = run_episode(model)\n",
    "    update(model, log_probs, rewards)\n",
    "\n",
    "    rewards_track += [sum(rewards).detach().tolist()]\n",
    "    steps_per_episode += [len(actions)]\n",
    "    landed += [observations[-1, -2:].sum() == 2]\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        ax.clear()\n",
    "        ax.plot(rewards_track)\n",
    "        ax.set_title(\"Reward\")\n",
    "\n",
    "        # add a smoothed version\n",
    "        if len(rewards_track) > 100:\n",
    "            smoothed = [np.mean(rewards_track[i:i+100]) for i in range(len(rewards_track) - 100)]\n",
    "            ax.plot(smoothed)\n",
    "\n",
    "        # also plot the number of steps\n",
    "        ax2.plot(steps_per_episode, color='red')\n",
    "        ax2.set_title(\"Steps per episode\")\n",
    "\n",
    "        # and the number of times we landed\n",
    "        ax3.set_title(\"Landed\")\n",
    "        if len(landed) > 30:\n",
    "            smoothed = [np.mean(landed[i:i+30]) for i in range(len(landed) - 30)]\n",
    "            ax3.plot(smoothed, color='green')\n",
    "\n",
    "        display(fig)\n",
    "        clear_output(wait=True)\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if sum(rewards_track[-100:]) / 100 > 195:\n",
    "            print(\"Solved!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
